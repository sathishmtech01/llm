{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0e3203-e9af-4fcd-9bed-a912642c03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain/graph/smith\n",
    "## - https://python.langchain.com/v0.1/docs/integrations/chat/groq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57c19d38-e82e-45a7-bd8e-a5a819c16766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a4a73a-54e5-4c77-bcc4-7e7bfcf452d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4617d546-5a7d-4b5d-9797-159e130686db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Groq Api Key ········\n"
     ]
    }
   ],
   "source": [
    "roq_api_key = getpass.getpass(prompt='Groq Api Key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0972c24d-6475-4d1c-a899-bed7c31bd335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name=\"mixtral-8x7b-32768\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfc6693f-1206-403b-b08f-367241d300b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Low latency Large Language Models (LLMs) are important for a variety of reasons, particularly in applications that require real-time or near real-time interactions. Here are some of the key reasons why low latency is important in LLMs:\\n\\n1. Improved user experience: In applications such as chatbots, search engines, and virtual assistants, users expect quick and responsive interactions. Low latency LLMs can provide instantaneous responses, which can significantly improve the user experience and increase user satisfaction.\\n2. Real-time decision making: In some applications, such as autonomous vehicles, real-time decision making is critical for safety and efficiency. Low latency LLMs can provide quick and accurate responses, enabling real-time decision making and improving overall system performance.\\n3. Reduced computational cost: Low latency LLMs can reduce the computational cost of running language models, as they require fewer computational resources to provide quick responses. This can lead to cost savings for businesses and organizations that rely on LLMs for their operations.\\n4. Improved model accuracy: Low latency LLMs can improve the accuracy of language models by reducing the time it takes to process and analyze input data. This can lead to more accurate and relevant responses, improving the overall performance of the LLM.\\n5. Enhanced competitiveness: In today's fast-paced digital world, businesses and organizations that can provide quick and responsive services have a competitive advantage. Low latency LLMs can enable businesses and organizations to provide real-time or near real-time interactions, giving them an edge over their competitors.\\n\\nOverall, low latency LLMs are critical for applications that require real-time or near real-time interactions, as they can improve user experience, enable real-time decision making, reduce computational cost, improve model accuracy, and enhance competitiveness.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 393, 'prompt_tokens': 25, 'total_tokens': 418, 'completion_time': 0.626278756, 'prompt_time': 0.002758282, 'queue_time': 0.011456106, 'total_time': 0.629037038}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-e5d3350e-44ce-4f53-b570-ef99437687bf-0', usage_metadata={'input_tokens': 25, 'output_tokens': 393, 'total_tokens': 418})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0c7629e-b1ec-4ae5-9d7a-d8dbcff93a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"There's a bright ball of gas in the sky,\\nThe Sun, so very difficult to deny.\\nIt rises and sets,\\nIn a daily bet,\\nOf light and of warmth, never shy.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 16, 'total_tokens': 62, 'completion_time': 0.071379437, 'prompt_time': 0.002163193, 'queue_time': 0.011423695000000001, 'total_time': 0.07354263}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-ec6dff81-2638-4efb-b6c7-e38d7cca97c1-0', usage_metadata={'input_tokens': 16, 'output_tokens': 46, 'total_tokens': 62})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a Limerick about {topic}\")])\n",
    "chain = prompt | chat\n",
    "await chain.ainvoke({\"topic\": \"The Sun\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6457b904-9a8b-4c0b-a472-9b4ee5dd41ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent, luminous,\n",
      "Glowing in the velvet night,\n",
      "The moon's gentle light."
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([(\"human\", \"Write a haiku about {topic}\")])\n",
    "chain = prompt | chat\n",
    "for chunk in chain.stream({\"topic\": \"The Moon\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cc8d2-f60c-4acc-be94-7374d6c601db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
