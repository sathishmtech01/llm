{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 5793551,
          "sourceType": "datasetVersion",
          "datasetId": 3327967
        },
        {
          "sourceId": 6369513,
          "sourceType": "datasetVersion",
          "datasetId": 3669776
        }
      ],
      "dockerImageVersionId": 30498,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathishmtech01/llm/blob/main/notebooks/QA_model_LLM_CSK_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "- Use [Langchain](https://python.langchain.com/en/latest/index.html) to **<font color='orange'>build a chatbot that can answer questions about</font>** [Harry Potter books](https://www.kaggle.com/datasets/hinepo/harry-potter-books-in-pdf-1-7)\n",
        "- **<font color='orange'>Flexible and customizable RAG pipeline (Retrieval Augmented Generation)</font>**\n",
        "- Experiment with various LLMs (Large Language Models)\n",
        "- Use [FAISS vector store](https://python.langchain.com/docs/integrations/vectorstores/faiss) to store text embeddings created with [Sentence Transformers](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) from ðŸ¤—. FAISS runs on GPU and it is much faster than Chroma\n",
        "- Use [Retrieval chain](https://python.langchain.com/docs/modules/data_connection/retrievers/) to retrieve relevant passages from embedded text\n",
        "- Summarize retrieved passages\n",
        "- Leverage Kaggle dual GPU (2 * T4) with [Hugging Face Accelerate](https://huggingface.co/docs/accelerate/index)\n",
        "- Chat UI with [Gradio](https://www.gradio.app/guides/quickstart)\n",
        "\n",
        "**<font color='green'>No need to create any API key to use this notebook! Everything is open source.</font>**\n",
        "\n",
        "- Colad - T4 instance\n",
        "\n",
        "\n",
        "### Models\n",
        "\n",
        "- [TheBloke/wizardLM-7B-HF](https://huggingface.co/TheBloke/wizardLM-7B-HF)\n",
        "- [daryl149/llama-2-7b-chat-hf](https://huggingface.co/daryl149/llama-2-7b-chat-hf)\n",
        "- [daryl149/llama-2-13b-chat-hf](https://huggingface.co/daryl149/llama-2-13b-chat-hf)\n",
        "- [mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)"
      ],
      "metadata": {
        "id": "PRFeEAlBeEAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:ceef601b-8cca-48a5-a433-54c0070f1f44.png)\n",
        "\n",
        "img source: HinePo"
      ],
      "metadata": {
        "id": "KHFnMwgaeEAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! nvidia-smi -L"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:49:04.98247Z",
          "iopub.execute_input": "2024-03-03T04:49:04.98292Z",
          "iopub.status.idle": "2024-03-03T04:49:05.985586Z",
          "shell.execute_reply.started": "2024-03-03T04:49:04.982886Z",
          "shell.execute_reply": "2024-03-03T04:49:05.984516Z"
        },
        "trusted": true,
        "id": "RNhISDgleEAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "qv9Usjh5eEAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "! pip install sentence_transformers==2.2.2\n",
        "\n",
        "! pip install -qq -U langchain\n",
        "! pip install -qq -U tiktoken\n",
        "! pip install -qq -U pypdf\n",
        "! pip install -qq -U faiss-gpu\n",
        "! pip install -qq -U InstructorEmbedding\n",
        "\n",
        "! pip install -qq -U transformers\n",
        "! pip install -qq -U accelerate\n",
        "! pip install -qq -U bitsandbytes\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:49:05.988165Z",
          "iopub.execute_input": "2024-03-03T04:49:05.98858Z",
          "iopub.status.idle": "2024-03-03T04:50:50.589987Z",
          "shell.execute_reply.started": "2024-03-03T04:49:05.988537Z",
          "shell.execute_reply": "2024-03-03T04:50:50.588886Z"
        },
        "trusted": true,
        "id": "g2UKCGcieEAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes"
      ],
      "metadata": {
        "id": "cXPFqUIfiE-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "tHNCWfiseEAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import textwrap\n",
        "import time\n",
        "\n",
        "import langchain\n",
        "\n",
        "### loaders\n",
        "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "\n",
        "### splits\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "### prompts\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "### vector stores\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "### models\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "### retrievers\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline\n",
        ")\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:50:50.591532Z",
          "iopub.execute_input": "2024-03-03T04:50:50.591912Z",
          "iopub.status.idle": "2024-03-03T04:51:03.032318Z",
          "shell.execute_reply.started": "2024-03-03T04:50:50.591881Z",
          "shell.execute_reply": "2024-03-03T04:51:03.03131Z"
        },
        "trusted": true,
        "id": "u5xw33M9eEAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('langchain:', langchain.__version__)\n",
        "print('torch:', torch.__version__)\n",
        "print('transformers:', transformers.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.033381Z",
          "iopub.execute_input": "2024-03-03T04:51:03.033987Z",
          "iopub.status.idle": "2024-03-03T04:51:03.039577Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.033959Z",
          "shell.execute_reply": "2024-03-03T04:51:03.038619Z"
        },
        "trusted": true,
        "id": "6KNwCwGDeEAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sorted(glob.glob('/kaggle/input/harry-potter-books-in-pdf-1-7/HP books/*'))\n",
        "sorted(glob.glob('llm/python_tutorial.pdf'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.041973Z",
          "iopub.execute_input": "2024-03-03T04:51:03.042258Z",
          "iopub.status.idle": "2024-03-03T04:51:03.063129Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.042232Z",
          "shell.execute_reply": "2024-03-03T04:51:03.062135Z"
        },
        "trusted": true,
        "id": "ehIYwrfyeEAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CFG\n",
        "\n",
        "- CFG class enables easy and organized experimentation"
      ],
      "metadata": {
        "id": "9HIdHQlbeEAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    # LLMs\n",
        "    model_name = 'llama2-13b-chat' # wizardlm, llama2-7b-chat, llama2-13b-chat, mistral-7B\n",
        "    temperature = 0\n",
        "    top_p = 0.95\n",
        "    repetition_penalty = 1.15\n",
        "\n",
        "    # splitting\n",
        "    split_chunk_size = 800\n",
        "    split_overlap = 0\n",
        "\n",
        "    # embeddings\n",
        "    embeddings_model_repo = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "    # similar passages\n",
        "    k = 6\n",
        "\n",
        "    # paths\n",
        "    PDFs_path = 'llm/'\n",
        "    Embeddings_path =  'llm/faiss-hp-sentence-transformers'\n",
        "    Output_folder = 'llm/vectordb'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.064422Z",
          "iopub.execute_input": "2024-03-03T04:51:03.064727Z",
          "iopub.status.idle": "2024-03-03T04:51:03.069946Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.064702Z",
          "shell.execute_reply": "2024-03-03T04:51:03.069047Z"
        },
        "trusted": true,
        "id": "IVWWtJhKeEAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define model"
      ],
      "metadata": {
        "id": "8ypONxF4eEAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model = CFG.model_name):\n",
        "\n",
        "    print('\\nDownloading model: ', model, '\\n\\n')\n",
        "\n",
        "    if model == 'wizardlm':\n",
        "        model_repo = 'TheBloke/wizardLM-7B-HF'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True\n",
        "        )\n",
        "\n",
        "        max_len = 1024\n",
        "\n",
        "    elif model == 'llama2-7b-chat':\n",
        "        model_repo = 'daryl149/llama-2-7b-chat-hf'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "            trust_remote_code = True\n",
        "        )\n",
        "\n",
        "        max_len = 2048\n",
        "\n",
        "    elif model == 'llama2-13b-chat':\n",
        "        model_repo = 'daryl149/llama-2-13b-chat-hf'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo, use_fast=True)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "            trust_remote_code = True\n",
        "        )\n",
        "\n",
        "        max_len = 2048 # 8192\n",
        "\n",
        "    elif model == 'mistral-7B':\n",
        "        model_repo = 'mistralai/Mistral-7B-v0.1'\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
        "\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit = True,\n",
        "            bnb_4bit_quant_type = \"nf4\",\n",
        "            bnb_4bit_compute_dtype = torch.float16,\n",
        "            bnb_4bit_use_double_quant = True,\n",
        "        )\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_repo,\n",
        "            quantization_config = bnb_config,\n",
        "            device_map = 'auto',\n",
        "            low_cpu_mem_usage = True,\n",
        "        )\n",
        "\n",
        "        max_len = 1024\n",
        "\n",
        "    else:\n",
        "        print(\"Not implemented model (tokenizer and backbone)\")\n",
        "\n",
        "    return tokenizer, model, max_len"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.071129Z",
          "iopub.execute_input": "2024-03-03T04:51:03.071375Z",
          "iopub.status.idle": "2024-03-03T04:51:03.084886Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.071353Z",
          "shell.execute_reply": "2024-03-03T04:51:03.084045Z"
        },
        "trusted": true,
        "id": "HrpSbfYieEAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "tokenizer, model, max_len = get_model(model = CFG.model_name)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:51:03.087567Z",
          "iopub.execute_input": "2024-03-03T04:51:03.08786Z",
          "iopub.status.idle": "2024-03-03T04:53:23.802829Z",
          "shell.execute_reply.started": "2024-03-03T04:51:03.087835Z",
          "shell.execute_reply": "2024-03-03T04:53:23.801631Z"
        },
        "trusted": true,
        "id": "L9L67OjweEAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.80415Z",
          "iopub.execute_input": "2024-03-03T04:53:23.8045Z",
          "iopub.status.idle": "2024-03-03T04:53:23.819144Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.804468Z",
          "shell.execute_reply": "2024-03-03T04:53:23.818054Z"
        },
        "trusted": true,
        "id": "-UNII4MaeEAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### check how Accelerate split the model across the available devices (GPUs)\n",
        "model.hf_device_map"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.820482Z",
          "iopub.execute_input": "2024-03-03T04:53:23.820832Z",
          "iopub.status.idle": "2024-03-03T04:53:23.834429Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.820802Z",
          "shell.execute_reply": "2024-03-03T04:53:23.833281Z"
        },
        "trusted": true,
        "id": "FIRPsu-heEAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤— pipeline\n",
        "\n",
        "- Hugging Face pipeline"
      ],
      "metadata": {
        "id": "hSK-zXOQeEAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### hugging face pipeline\n",
        "pipe = pipeline(\n",
        "    task = \"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    pad_token_id = tokenizer.eos_token_id,\n",
        "#     do_sample = True,\n",
        "    max_length = max_len,\n",
        "    temperature = CFG.temperature,\n",
        "    top_p = CFG.top_p,\n",
        "    repetition_penalty = CFG.repetition_penalty\n",
        ")\n",
        "\n",
        "### langchain pipeline\n",
        "llm = HuggingFacePipeline(pipeline = pipe)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.835898Z",
          "iopub.execute_input": "2024-03-03T04:53:23.836219Z",
          "iopub.status.idle": "2024-03-03T04:53:23.845654Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.836191Z",
          "shell.execute_reply": "2024-03-03T04:53:23.844799Z"
        },
        "trusted": true,
        "id": "jI_Z3wejeEAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.846908Z",
          "iopub.execute_input": "2024-03-03T04:53:23.847216Z",
          "iopub.status.idle": "2024-03-03T04:53:23.857226Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.847189Z",
          "shell.execute_reply": "2024-03-03T04:53:23.856223Z"
        },
        "trusted": true,
        "id": "OkKKQiAAeEAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "### testing model, not using the harry potter books yet\n",
        "### answer is not necessarily related to harry potter\n",
        "query = \"what is bert fp\"\n",
        "llm.invoke(query)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:53:23.858245Z",
          "iopub.execute_input": "2024-03-03T04:53:23.858505Z",
          "iopub.status.idle": "2024-03-03T04:54:05.862672Z",
          "shell.execute_reply.started": "2024-03-03T04:53:23.858473Z",
          "shell.execute_reply": "2024-03-03T04:54:05.861701Z"
        },
        "trusted": true,
        "id": "G5oBTpJSeEAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¦œðŸ”— Langchain\n",
        "\n",
        "- Multiple document retriever with LangChain"
      ],
      "metadata": {
        "id": "9gToNsOAeEAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CFG.model_name"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:54:05.867225Z",
          "iopub.execute_input": "2024-03-03T04:54:05.867536Z",
          "iopub.status.idle": "2024-03-03T04:54:05.873505Z",
          "shell.execute_reply.started": "2024-03-03T04:54:05.867507Z",
          "shell.execute_reply": "2024-03-03T04:54:05.872533Z"
        },
        "trusted": true,
        "id": "FgXBW3HkeEAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loader\n",
        "\n",
        "- [Directory loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory) for multiple files\n",
        "- This step is not necessary if you are just loading the vector database\n",
        "- This step is necessary if you are creating embeddings. In this case you need to:\n",
        "    - load de PDF files\n",
        "    - split into chunks\n",
        "    - create embeddings\n",
        "    - save the embeddings in a vector store\n",
        "    - After that you can just load the saved embeddings to do similarity search with the user query, and then use the LLM to answer the question\n",
        "    \n",
        "You can comment out this section if you use the embeddings I already created."
      ],
      "metadata": {
        "id": "nCfM4xLIeEAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "loader = DirectoryLoader(\n",
        "    CFG.PDFs_path,\n",
        "    glob=\"./*.pdf\",\n",
        "    loader_cls=PyPDFLoader,\n",
        "    show_progress=True,\n",
        "    use_multithreading=True\n",
        ")\n",
        "\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:54:05.874703Z",
          "iopub.execute_input": "2024-03-03T04:54:05.875112Z",
          "iopub.status.idle": "2024-03-03T04:56:26.454696Z",
          "shell.execute_reply.started": "2024-03-03T04:54:05.875077Z",
          "shell.execute_reply": "2024-03-03T04:56:26.453617Z"
        },
        "trusted": true,
        "id": "gvrjr9m8eEAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'We have {len(documents)} pages in total')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:26.456408Z",
          "iopub.execute_input": "2024-03-03T04:56:26.45721Z",
          "iopub.status.idle": "2024-03-03T04:56:26.462709Z",
          "shell.execute_reply.started": "2024-03-03T04:56:26.457168Z",
          "shell.execute_reply": "2024-03-03T04:56:26.461769Z"
        },
        "trusted": true,
        "id": "HIOMJgJveEAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents[8].page_content"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:26.464164Z",
          "iopub.execute_input": "2024-03-03T04:56:26.464473Z",
          "iopub.status.idle": "2024-03-03T04:56:26.476236Z",
          "shell.execute_reply.started": "2024-03-03T04:56:26.464446Z",
          "shell.execute_reply": "2024-03-03T04:56:26.475207Z"
        },
        "trusted": true,
        "id": "tUE5XmMQeEAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitter\n",
        "\n",
        "- Splitting the text into chunks so its passages are easily searchable for similarity\n",
        "- This step is also only necessary if you are creating the embeddings\n",
        "- [RecursiveCharacterTextSplitter](https://python.langchain.com/en/latest/reference/modules/document_loaders.html?highlight=RecursiveCharacterTextSplitter#langchain.document_loaders.MWDumpLoader)"
      ],
      "metadata": {
        "id": "KMM2-38feEAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = CFG.split_chunk_size,\n",
        "    chunk_overlap = CFG.split_overlap\n",
        ")\n",
        "\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f'We have created {len(texts)} chunks from {len(documents)} pages')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:26.47748Z",
          "iopub.execute_input": "2024-03-03T04:56:26.477817Z",
          "iopub.status.idle": "2024-03-03T04:56:27.592563Z",
          "shell.execute_reply.started": "2024-03-03T04:56:26.477778Z",
          "shell.execute_reply": "2024-03-03T04:56:27.591429Z"
        },
        "trusted": true,
        "id": "alFkLvdteEAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Embeddings\n",
        "\n",
        "\n",
        "- Embedd and store the texts in a Vector database (FAISS)\n",
        "- [LangChain Vector Stores docs](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
        "- [FAISS - langchain](https://python.langchain.com/docs/integrations/vectorstores/faiss)\n",
        "- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks - paper Aug/2019](https://arxiv.org/pdf/1908.10084.pdf)\n",
        "- [This is a nice 4 minutes video about vector stores](https://www.youtube.com/watch?v=dN0lsF2cvm4)\n",
        "\n",
        "___\n",
        "\n",
        "- If you use Chroma vector store it will take ~35 min to create embeddings\n",
        "- If you use FAISS vector store on GPU it will take just ~3 min\n",
        "\n",
        "___\n",
        "\n",
        "We need to create the embeddings only once, and then we can just load the vector store and query the database using similarity search.\n",
        "\n",
        "Loading the embeddings takes only a few seconds.\n",
        "\n",
        "I uploaded the embeddings to a Kaggle Dataset so we just load it from [here](https://www.kaggle.com/datasets/hinepo/faiss-hp-sentence-transformers)."
      ],
      "metadata": {
        "id": "qy55zPVMeEAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "### we create the embeddings only if they do not exist yet\n",
        "if not os.path.exists(CFG.Embeddings_path + '/index.faiss'):\n",
        "\n",
        "    ### download embeddings model\n",
        "    embeddings = HuggingFaceInstructEmbeddings(\n",
        "        model_name = CFG.embeddings_model_repo,\n",
        "        model_kwargs = {\"device\": \"cuda\"}\n",
        "    )\n",
        "\n",
        "    ### create embeddings and DB\n",
        "    vectordb = FAISS.from_documents(\n",
        "        documents = texts,\n",
        "        embedding = embeddings\n",
        "    )\n",
        "\n",
        "    ### persist vector database\n",
        "    vectordb.save_local(f\"{CFG.Output_folder}/faiss_index_hp\") # save in output folder\n",
        "#     vectordb.save_local(f\"{CFG.Embeddings_path}/faiss_index_hp\") # save in input folder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:27.594158Z",
          "iopub.execute_input": "2024-03-03T04:56:27.5946Z",
          "iopub.status.idle": "2024-03-03T04:56:27.603829Z",
          "shell.execute_reply.started": "2024-03-03T04:56:27.594561Z",
          "shell.execute_reply": "2024-03-03T04:56:27.602693Z"
        },
        "trusted": true,
        "id": "EGPyNC8XeEAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If creating embeddings, remember that on Kaggle we can not write data to the input folder.\n",
        "\n",
        "So just write (save) the embeddings to the output folder and then load them from there."
      ],
      "metadata": {
        "id": "CV2xzCGveEAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load vector database\n",
        "\n",
        "- After saving the vector database, we just load it from the Kaggle Dataset I mentioned\n",
        "- Obviously, the embeddings function to load the embeddings must be the same as the one used to create the embeddings"
      ],
      "metadata": {
        "id": "c9SZ7_FGeEAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "### download embeddings model\n",
        "embeddings = HuggingFaceInstructEmbeddings(\n",
        "    model_name = CFG.embeddings_model_repo,\n",
        "    model_kwargs = {\"device\": \"cuda\"}\n",
        ")\n",
        "\n",
        "### load vector DB embeddings\n",
        "vectordb = FAISS.load_local(\n",
        "    # CFG.Embeddings_path, # from input folder\n",
        "    CFG.Output_folder + '/faiss_index_hp', # from output folder\n",
        "    embeddings,\n",
        "    allow_dangerous_deserialization=True\n",
        ")\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:27.605407Z",
          "iopub.execute_input": "2024-03-03T04:56:27.605867Z",
          "iopub.status.idle": "2024-03-03T04:56:29.883531Z",
          "shell.execute_reply.started": "2024-03-03T04:56:27.605836Z",
          "shell.execute_reply": "2024-03-03T04:56:29.882458Z"
        },
        "trusted": true,
        "id": "zsLu3_uMeEAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### test if vector DB was loaded correctly\n",
        "vectordb.similarity_search('contional loop')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:29.884796Z",
          "iopub.execute_input": "2024-03-03T04:56:29.885122Z",
          "iopub.status.idle": "2024-03-03T04:56:30.402706Z",
          "shell.execute_reply.started": "2024-03-03T04:56:29.885092Z",
          "shell.execute_reply": "2024-03-03T04:56:30.401809Z"
        },
        "trusted": true,
        "id": "plAFZb1FeEAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template\n",
        "\n",
        "- Custom prompt"
      ],
      "metadata": {
        "id": "H2Df0S9BeEAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "Don't try to make up an answer, if you don't know just say that you don't know.\n",
        "Answer in the same language the question was asked.\n",
        "Use only the following pieces of context to answer the question at the end.\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template = prompt_template,\n",
        "    input_variables = [\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.403842Z",
          "iopub.execute_input": "2024-03-03T04:56:30.404115Z",
          "iopub.status.idle": "2024-03-03T04:56:30.409596Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.404091Z",
          "shell.execute_reply": "2024-03-03T04:56:30.408569Z"
        },
        "trusted": true,
        "id": "KQaaCXjveEAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm_chain = LLMChain(prompt=PROMPT, llm=llm)\n",
        "# llm_chain"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.410886Z",
          "iopub.execute_input": "2024-03-03T04:56:30.411175Z",
          "iopub.status.idle": "2024-03-03T04:56:30.421165Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.41115Z",
          "shell.execute_reply": "2024-03-03T04:56:30.420262Z"
        },
        "trusted": true,
        "id": "Let34vQAeEAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever chain\n",
        "\n",
        "- Retriever to retrieve relevant passages\n",
        "- Chain to answer questions\n",
        "- [RetrievalQA: Chain for question-answering](https://python.langchain.com/docs/modules/data_connection/retrievers/)"
      ],
      "metadata": {
        "id": "3CcGqf6aeEAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs = {\"k\": CFG.k, \"search_type\" : \"similarity\"})\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm = llm,\n",
        "    chain_type = \"stuff\", # map_reduce, map_rerank, stuff, refine\n",
        "    retriever = retriever,\n",
        "    chain_type_kwargs = {\"prompt\": PROMPT},\n",
        "    return_source_documents = True,\n",
        "    verbose = False\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.42236Z",
          "iopub.execute_input": "2024-03-03T04:56:30.422661Z",
          "iopub.status.idle": "2024-03-03T04:56:30.43323Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.422625Z",
          "shell.execute_reply": "2024-03-03T04:56:30.432288Z"
        },
        "trusted": true,
        "id": "nfLvPMhEeEAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### testing MMR search\n",
        "question = \"Which is inheritance?\"\n",
        "vectordb.max_marginal_relevance_search(question, k = CFG.k)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.434349Z",
          "iopub.execute_input": "2024-03-03T04:56:30.434654Z",
          "iopub.status.idle": "2024-03-03T04:56:30.495228Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.434628Z",
          "shell.execute_reply": "2024-03-03T04:56:30.494286Z"
        },
        "trusted": true,
        "id": "-iK7LaeBeEAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### testing similarity search\n",
        "question = \"Which is function?\"\n",
        "vectordb.similarity_search(question, k = CFG.k)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.496296Z",
          "iopub.execute_input": "2024-03-03T04:56:30.496552Z",
          "iopub.status.idle": "2024-03-03T04:56:30.522625Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.496528Z",
          "shell.execute_reply": "2024-03-03T04:56:30.52166Z"
        },
        "trusted": true,
        "id": "rPvetgOTeEAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post-process outputs\n",
        "\n",
        "- Format llm response\n",
        "- Cite sources (PDFs)\n",
        "- Change `width` parameter to format the output"
      ],
      "metadata": {
        "id": "iGulzFZReEAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wrap_text_preserve_newlines(text, width=700):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    ans = wrap_text_preserve_newlines(llm_response['result'])\n",
        "\n",
        "    sources_used = ' \\n'.join(\n",
        "        [\n",
        "            source.metadata['source'].split('/')[-1][:-4]\n",
        "            + ' - page: '\n",
        "            + str(source.metadata['page'])\n",
        "            for source in llm_response['source_documents']\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    ans = ans + '\\n\\nSources: \\n' + sources_used\n",
        "    return ans"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.5241Z",
          "iopub.execute_input": "2024-03-03T04:56:30.524496Z",
          "iopub.status.idle": "2024-03-03T04:56:30.532112Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.524459Z",
          "shell.execute_reply": "2024-03-03T04:56:30.531045Z"
        },
        "trusted": true,
        "id": "W7hRO-TKeEAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llm_ans(query):\n",
        "    start = time.time()\n",
        "\n",
        "    llm_response = qa_chain.invoke(query)\n",
        "    ans = process_llm_response(llm_response)\n",
        "\n",
        "    end = time.time()\n",
        "\n",
        "    time_elapsed = int(round(end - start, 0))\n",
        "    time_elapsed_str = f'\\n\\nTime elapsed: {time_elapsed} s'\n",
        "    return ans + time_elapsed_str"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.533262Z",
          "iopub.execute_input": "2024-03-03T04:56:30.533556Z",
          "iopub.status.idle": "2024-03-03T04:56:30.542074Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.533529Z",
          "shell.execute_reply": "2024-03-03T04:56:30.541111Z"
        },
        "trusted": true,
        "id": "tHhyEoX2eEAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ask questions\n",
        "\n",
        "- Question Answering from multiple documents\n",
        "- Invoke QA Chain\n",
        "- Talk to your data"
      ],
      "metadata": {
        "id": "SGI6LglIeEAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CFG.model_name"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.543255Z",
          "iopub.execute_input": "2024-03-03T04:56:30.54353Z",
          "iopub.status.idle": "2024-03-03T04:56:30.552891Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.543505Z",
          "shell.execute_reply": "2024-03-03T04:56:30.551908Z"
        },
        "trusted": true,
        "id": "LRUVu-8XeEAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"while loop\"\n",
        "print(llm_ans(query))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:30.554201Z",
          "iopub.execute_input": "2024-03-03T04:56:30.55449Z",
          "iopub.status.idle": "2024-03-03T04:56:44.068201Z",
          "shell.execute_reply.started": "2024-03-03T04:56:30.554464Z",
          "shell.execute_reply": "2024-03-03T04:56:44.067025Z"
        },
        "trusted": true,
        "id": "u1ke-ZJBeEAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Author of the book?\"\n",
        "print(llm_ans(query))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:44.069536Z",
          "iopub.execute_input": "2024-03-03T04:56:44.069867Z",
          "iopub.status.idle": "2024-03-03T04:56:48.40032Z",
          "shell.execute_reply.started": "2024-03-03T04:56:44.069838Z",
          "shell.execute_reply": "2024-03-03T04:56:48.399339Z"
        },
        "trusted": true,
        "id": "0O2BuFskeEAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Exception handling\"\n",
        "print(llm_ans(query))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-03T04:56:48.401764Z",
          "iopub.execute_input": "2024-03-03T04:56:48.402112Z",
          "iopub.status.idle": "2024-03-03T04:56:57.075701Z"
        },
        "trusted": true,
        "id": "Hl9dCcVbeEAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"regular expression\"\n",
        "print(llm_ans(query))"
      ],
      "metadata": {
        "trusted": true,
        "id": "D8eHGCcgeEAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Chat UI\n",
        "\n",
        "- **<font color='orange'>At the moment this part only works on Google Colab. Gradio and Kaggle started having compatibility issues recently.</font>**\n",
        "- If you plan to use the interface, it is preferable to do so in Google Colab\n",
        "- I'll leave this section commented out for now\n",
        "- Chat UI prints below\n",
        "\n",
        "___\n",
        "\n",
        "- Create a chat UI with [Gradio](https://www.gradio.app/guides/quickstart)\n",
        "- [ChatInterface docs](https://www.gradio.app/docs/chatinterface)\n",
        "- The notebook should be running if you want to use the chat interface"
      ],
      "metadata": {
        "id": "uIdqL_SceEAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "trusted": true,
        "id": "y-V8MOT1eEAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade gradio -qq\n",
        "clear_output()"
      ],
      "metadata": {
        "trusted": true,
        "id": "nC5-pRWgeEAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "print(gr.__version__)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RAeNqfMaeEAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(message, history):\n",
        "    # output = message # debug mode\n",
        "\n",
        "    output = str(llm_ans(message)).replace(\"\\n\", \"<br/>\")\n",
        "    return output\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    predict,\n",
        "    title = f' Open-Source LLM ({CFG.model_name}) for Question Answering'\n",
        ")\n",
        "\n",
        "demo.queue()\n",
        "demo.launch()"
      ],
      "metadata": {
        "trusted": true,
        "id": "OmXUJWMQeEAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:413fe7a3-6534-45b5-b6e3-7fc86e982cf1.png)"
      ],
      "metadata": {
        "id": "V6EPqlLgeEAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:976f4bf4-7626-4d4a-b773-3eebd7e9f000.png)"
      ],
      "metadata": {
        "id": "RoMryjCHeEAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "- Feel free to fork and optimize the code. Lots of things can be improved.\n",
        "\n",
        "- Things I found had the most impact on models output quality in my experiments:\n",
        "    - Prompt engineering\n",
        "    - Bigger models\n",
        "    - Other models families\n",
        "    - Splitting: chunk size, overlap\n",
        "    - Search: Similarity, MMR, k\n",
        "    - Pipeline parameters (temperature, top_p, penalty)\n",
        "    - Embeddings function\n",
        "    - LLM parameters (max len)\n",
        "\n",
        "\n",
        "- LangChain, Hugging Face and Gradio are awesome libs!\n",
        "\n",
        "- **<font color='orange'>If you liked this notebook, don't forget to show your support with an Upvote!</font>**\n",
        "\n",
        "- In case you are interested in LLMs, I also have some other notebooks you might want to check:\n",
        "\n",
        "    - [Instruction Finetuning](https://www.kaggle.com/code/hinepo/llm-instruction-finetuning-wandb)\n",
        "    - [Preference Finetuning - LLM Alignment](https://www.kaggle.com/code/hinepo/llm-alignment-preference-finetuning)\n",
        "    - [Synthetic Data for Finetuning](https://www.kaggle.com/code/hinepo/synthetic-data-creation-for-llms)\n",
        "    - [Safeguards and Guardrails](https://www.kaggle.com/code/hinepo/llm-safeguards-and-guardrails)\n",
        "    \n",
        "___\n",
        "\n",
        "ðŸ¦œðŸ”—ðŸ¤—"
      ],
      "metadata": {
        "id": "w7Z6mt8feEAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:68773819-4358-4ded-be3e-f1d275103171.png)"
      ],
      "metadata": {
        "id": "eE6x3OlXeEAz"
      }
    }
  ]
}